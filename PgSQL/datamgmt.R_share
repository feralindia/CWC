## This script is to organise the various data sets throuth PostgreSQL and PostGIS
## It automatically transfers unique datasets from csv files to database tables.
library(RPostgreSQL)
## Define the connection
con <- dbConnect(PostgreSQL(), user= "USERNAME", password="PASSWORD", dbname="DATABASE");
tbrgdatadir<-"/home/rsb/OngoingProjects/CWC/Data/tbrg" ## Fix this to your requirements
wlrdatadir<-"/home/rsb/OngoingProjects/CWC/Data/wlr/" ## Fix this to your requirements
wkdir<-"/home/rsb/OngoingProjects/WGP/Themes/HydroGRASS101/rdata/"  ## Fix this to your requirements
setwd(wkdir)
## Create tables to hold the tbrg and wlr datasets
num_tbrg<-101:130 # the number of tbrgs
## Ensure the diretories for the data are created
## in the console "mkdir tbrg_{101..130}" will create directories tbrg_101 to tbrg_130
i=1
while(i<=length(num_tbrg)){
  ## This statement is to create a table for each tbrg
  stm<-paste("CREATE TABLE IF NOT EXISTS nilgiris.tbrg_", 
                 num_tbrg[i], "_raw(date date, time time, volume REAL);", sep="")
  rs <- dbSendQuery(con, stm)
  rm(stm)
  ## This statement is to copy the data from the respective tbrg folders into the tables.
  ## Note that only CSV files are supported - don't stick in xls sheets.
  ## List the names of the files
  tbrgtab<-paste("tbrg_", num_tbrg[i], sep="") # Directory/table per tbrg holding csv files are stored.
  tbrgtab_raw<-paste("tbrg_", num_tbrg[i], "_raw", sep="")
  tbrgdir<-paste(tbrgdatadir, tbrgtab, sep="/") # Directory holding all tbrg sub folders
  dirliststm<-paste("ls ", tbrgdir, sep="") # List contents of tbrg## folder.
  filelist<-system(dirliststm, intern=TRUE) # Save the list of contents to an object
  ## run another loop within the earlier one
  ## This loop copies all the csv/dat files onto a raw tbrg table
  j=1
  while(j<=length(filelist)){
    stm<-paste("COPY nilgiris.tbrg_", num_tbrg[i], "_raw FROM '", 
               tbrgdir, "/", filelist[j], "' DELIMITER ',' CSV;", sep="")
    rs<-dbSendQuery(con,stm)
    rm(stm)
    j=j+1;
  }
 ## Create the tables to hold tbrg data - one per tbrg 
  stm<-paste("CREATE TABLE IF NOT EXISTS nilgiris.tbrg_", 
             num_tbrg[i], "(id SERIAL PRIMARY KEY, 
             date_time timestamp, volume real);", sep="")
  rs <- dbSendQuery(con, stm)
  rm(stm)
  ## Now transfer the data from the raw table to the clean table.
  stm<-paste("INSERT INTO nilgiris.", tbrgtab, "(date_time, volume) 
SELECT DISTINCT concat(nilgiris.", tbrgtab_raw, ".date, ' ', nilgiris.", tbrgtab_raw, ".time)::timestamp, 
nilgiris.", tbrgtab_raw, ".volume
FROM nilgiris.", tbrgtab_raw, " LEFT JOIN nilgiris.", tbrgtab, "
ON concat(nilgiris.", tbrgtab_raw, ".date, ' ', nilgiris.", tbrgtab_raw, ".time)::timestamp=", tbrgtab, ".date_time
AND nilgiris.", tbrgtab_raw, ".volume=nilgiris.", tbrgtab, ".volume
WHERE ", tbrgtab, ".date_time IS NULL AND ", tbrgtab, ".volume IS NULL;", sep="")
  rs <- dbSendQuery(con, stm)
  rm(stm)
  ## Remove the raw raw tables or they'll grow continuously.
  stm<-paste("DROP TABLE nilgiris.", tbrgtab_raw, ";", sep="")
  rs <- dbSendQuery(con, stm)
  rm(stm)
  ## VACUUM the tables to remove dead tuples
  stm<-paste("VACUUM nilgiris.", tbrgtab, ";", sep="")
  rs <- dbSendQuery(con, stm)
  rm(stm)
  i=i+1;
}
## Close the connection
postgresqlCloseConnection(con)
## Now repeat for WLRs

## Define the connection
con <- dbConnect(PostgreSQL(), user= "rsb", password="kar4nahi9", dbname="cwc");
## Read all CSV files but chop the first 10 lines which contain the headers

## tail -n +10 WLR104_104_001_31_05_2013.CSV > new.csv

## Create tables to hold the wlr datasets
num_wlr<-101:104 # the number of wlr
## Ensure the diretories for the data are created
## in the console "mkdir wlr{101..104}" will create directories wlr_101 to wlr_104
i=1
while(i<=length(num_wlr)){
  ## This statement is to create a table for each wlr
  stm<-paste("CREATE TABLE IF NOT EXISTS nilgiris.wlr_", 
             num_wlr[i], "_raw(scan integer, date date, time time, wl_raw real, wl_cal real);", sep="")
  rs <- dbSendQuery(con, stm)
  rm(stm)
  ## This statement is to copy the data from the respective wlr folders into the tables.
  ## Note that only CSV files are supported - don't stick in xls sheets.
  ## List the names of the files
  wlrtab<-paste("wlr_", num_wlr[i], sep="") # Database table for storing wlr data.
  wlrtab_raw<-paste("wlr_", num_wlr[i], "_raw", sep="")
  wlrdir<-paste(wlrdatadir, wlrtab, sep="") # Directory holding all wlr sub folders
  dirliststm<-paste("ls ", wlrdir, sep="") # List contents of wlr## folder.
  filelist<-system(dirliststm, intern=TRUE) # Save the list of contents to an object
  ## run another loop within the earlier one
  ## This loop copies all the csv/dat files onto a raw wlr table
  ## In a departure from the earlier script, remove the first ten lines (header)
  ## First move to the relevant directory.
  setwd(wlrdir)
  ## First change the datestyle to dmy
  stm<-paste("SET datestyle = 'ISO, DMY';")
  rs<-dbSendQuery(con,stm)
  rm(stm)
  j=1
  while(j<=length(filelist)){
    rmhead<-paste("tail -n +13", filelist[j], " > tmp.csv")
    system(rmhead)
    stm<-paste("COPY nilgiris.wlr_", num_wlr[i], "_raw FROM '", 
               wlrdir, "/tmp.csv' DELIMITER ',' CSV;", sep="")
    rs<-dbSendQuery(con,stm)
    rm(stm)
    system("rm tmp.csv")
    j=j+1;
  }
  ## Reset the datestyle to default
  stm<-paste("SET datestyle = default;")
  rs<-dbSendQuery(con,stm)
  rm(stm)
  ## Create the tables to hold wlr data - one per wlr 
  stm<-paste("CREATE TABLE IF NOT EXISTS nilgiris.wlr_", 
             num_wlr[i], "(id SERIAL PRIMARY KEY, scan integer, date_time timestamp, 
             wl_raw real, wl_cal real);", sep="")
  rs <- dbSendQuery(con, stm)
  rm(stm)
  ## Now transfer the data from the raw table to the clean table.
  stm<-paste("INSERT INTO nilgiris.", wlrtab, "(scan, date_time, wl_raw, wl_cal) 
             SELECT DISTINCT nilgiris.", wlrtab_raw, ".scan, 
concat(nilgiris.", wlrtab_raw, ".date, ' ', nilgiris.", wlrtab_raw, ".time)::timestamp, 
             nilgiris.", wlrtab_raw, ".wl_raw,  nilgiris.", wlrtab_raw, ".wl_cal
             FROM nilgiris.", wlrtab_raw, " LEFT JOIN nilgiris.", wlrtab, "
             ON concat(nilgiris.", wlrtab_raw, ".date, ' ', nilgiris.", wlrtab_raw, ".time)::timestamp=", wlrtab, ".date_time
             AND nilgiris.", wlrtab_raw, ".scan=nilgiris.", wlrtab, ".scan
             WHERE ", wlrtab, ".date_time IS NULL AND ", wlrtab, ".scan IS NULL;", sep="")
  rs <- dbSendQuery(con, stm)
  rm(stm)
  ## Remove the raw raw tables or they'll grow continuously.
  stm<-paste("DROP TABLE nilgiris.", wlrtab_raw, ";", sep="")
  rs <- dbSendQuery(con, stm)
  rm(stm)
  ## VACUUM the tables to remove dead tuples
  stm<-paste("VACUUM nilgiris.", wlrtab, ";", sep="")
  rs <- dbSendQuery(con, stm)
  rm(stm)
  i=i+1;
}
## Close the connection
postgresqlCloseConnection(con)
## Reset the working directory to default
setwd(wkdir)
## TODO
## Calibration files to be imported
## Code to calibrate data based on timestamp from calibration files
## Import temperature and RH data from buttons
## Create hydromad input files based on timestamp

